\chapter{Additional Features}
\section{Data Set}
\label{dataset}

The additional data set was recorded in the 7th district of ZÃ¼rich using a \emph{Huawei Nexus 6p} smartphone strapped to the luggage rack of a \emph{Vespa} motorcycle. 
Compared to the data sets provided to us it includes some additional difficulties that are handled by our VO implementation with varying success:\\

\begin{enumerate}
\item \emph{Additional Camera Movement:} Due to the size and weight of the motorcycle, the camera is affected by a much larger amount of vibration originating from the motor. 
The same factor also cause the vehicle to be subject to a lot more squat (leaning backwards on acceleration) and dive (leaning forward on breaking) than the cars used in the KITTI and the MALAGA data sets. 
This, together with leaning into turns, results in more camera rotation along the pitch and roll axes.

\item \emph{Traffic:} Another factor that adds to the complexity of our data set is traffic. 
Both pedestrian and vehicle traffic were present during the recording which includes two pedestrians walking by directly in front of the camera at some point.
Additionally, two stops at red lights were made.

\item \emph{Unknowns:} Preprocessing performed automatically on the video recording by the smartphone (e.g. image stabilization or autofocusing) adds an unpredictable factor that may affect the behavior of our VO pipeline.
\end{enumerate}

The VO pipeline is robust to factors \emph{1.} and \emph{3.}. 
The estimated path looks remains smooth and retains the overall direction well. 
\emph{2.} however, is an issue to some degree: While the trajectory is not significantly affected by other vehicles, pedestrians passing by in front of the camera lead to the loss of most of the key points causing the detection of erroneous movement.

\begin{figure}[htp]
\subfloat[a normal frame]{\includegraphics[width=.3\textwidth]{figures/vespa_0014.png}}\hfill
\subfloat[roll while making turns]{\includegraphics[width=.3\textwidth]{figures/vespa_0292.png}}\hfill
\subfloat[pedestrian traffic]{\includegraphics[width=.3\textwidth]{figures/vespa_1072.png}}
\caption{Frames from the \emph{Vespa} data set}
\end{figure}

\section{Bundle Adjustment}
\label{bundle adjustment}
As mentioned in section \ref{additionalWork}, the stand-alone VO pipeline suffers from a significant amount of scale drift. 
This means that along the trajectory, the average scale of the displacements changes. 
For continuous operation, a form of overlapping windowed BA was implemented to counteract this drift. \coderef{main.m, runBA.m} 
Every \baEveryNthFrame frames, the trajectory and landmarks are adjusted and replaced over a segment of the last \baReplaceframes frames. 
In order to ensure continuity, the adjusted point cloud and trajectory are constrained such that the last two orientations and locations of the last segment coincide with first two orientations and locations of the newly adjusted segment. \par
In cases where most key points are lost, BA switches to the \emph{Levenberg-Marquardt algorithm}. In these cases the locations and orientations must be aligned manually.  
Using BA, scale drift can be effectively counteracted as Fig. \ref{fig:BAeffect} shows.

% graphics of BA effect
\begin{figure}[htp]
  \centering
    \includegraphics[width=0.5\textwidth]{figures/BAeffect}
  \caption{Comparison between bundle adjusted trajectory (red) and non-bundle adjusted trajectory (blue). The red trajectory exhibits significantly lower scale drift.}
  \label{fig:BAeffect}
\end{figure}
% //////

\section{Ground Truth Alignment and Error Analysis}
\label{simulation}

For a stable operation of the VO pipeline, optimized parameters must be chosen to control detection and matching of new key points, triangulation etc. 
We determined these parameters by running a simulation \coderef {simMain.m, simTaskScheduler.m} on a benchmark data set (\emph{KITTI}) over various parameter choices and comparing the aligned \coderef{alignToGroundTruth.m} trajectories to the ground truth. The orientation and location errors were defined by the following formulae:

\begin{equation*}e_l = \underset{R,t,s}{\min} \sum_i (sR \cdot p_{est, i} + t - p_{GT, i})^2\end{equation*}
\begin{equation*}e_o = \sum_i \text{rotMatToRotVec}(R^* \cdot R_{est,i} \cdot R_{GT, i}^T)^2\end{equation*}

Where $i$ is the index along the trajectory, $e_l$ is the location error, $e_o$ is the orientation error, $p_{GT}$ and $R_{GT}$ are the ground truth position and rotation, $p_{est}$ and $R_{est}$ are the estimated position and rotation and $R^*$ is the argument for R when $e_p$ is minimal. The parameters $s$, $R$ and $t$ describe a generic similarity transformation. The function \emph{rotMatToRotVec} transforms the rotation matrix into a rotation vector of the form $\phi = \theta n$ \coderef{rotMatToRotVec.m}.  
Using these error metrics a detailed error analysis was performed over the first 500 frames. 
More information on this can be found in section \ref{performance}.